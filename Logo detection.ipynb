{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itpJh6hzGreP"
      },
      "source": [
        "# Logo detection with pre-trained CenterNet (hourglass104) model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKorQyGBe9js"
      },
      "source": [
        "## Prepare the working environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ejZRK6eTNul"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "ROOT_PATH = os.path.join(os.sep, 'content')\n",
        "TFOD_DIR_NAME = 'tf_object_detection'\n",
        "PRETRAINED_MODEL_NAME = 'centernet_hg104_512x512_coco17_tpu-8'\n",
        "\n",
        "paths = {\n",
        "    'DRIVE': os.path.join(ROOT_PATH, 'drive'),\n",
        "    'DATASET': os.path.join(ROOT_PATH, 'dataset'),\n",
        "    'LOGODET_3K': os.path.join(ROOT_PATH, 'dataset', 'LogoDet-3K'),\n",
        "    'LOGODET_3K_FLAT_SUBSET': os.path.join(ROOT_PATH, 'dataset', 'LogoDet_3K_flat_subset'),\n",
        "    'TRAIN_SET': os.path.join(ROOT_PATH, 'dataset', 'train'),\n",
        "    'TEST_SET': os.path.join(ROOT_PATH, 'dataset', 'test'),\n",
        "    'ANNOTATIONS': os.path.join(ROOT_PATH, 'annotations'),\n",
        "    'PRETRAINED_MODEL': os.path.join(ROOT_PATH, PRETRAINED_MODEL_NAME),\n",
        "    'OBJECT_DETECTION': os.path.join(ROOT_PATH, TFOD_DIR_NAME, 'research', 'object_detection'),\n",
        "}\n",
        "\n",
        "files = {\n",
        "    'LOGODET_3K': os.path.join(paths['DRIVE'], 'MyDrive', 'LogoDet-3K.zip'), # Edit this path to be the location of your dataset\n",
        "    'LABEL_MAP': os.path.join(paths['ANNOTATIONS'], 'label_map.pbtxt'),\n",
        "    'TRAINING_SCRIPT': os.path.join(paths['OBJECT_DETECTION'], 'model_main_tf2.py'),\n",
        "    'PIPELINE_CONFIG': os.path.join(ROOT_PATH, PRETRAINED_MODEL_NAME, 'pipeline.config'),\n",
        "}\n",
        "\n",
        "if not os.path.isdir(paths['ANNOTATIONS']):\n",
        "    os.mkdir(paths['ANNOTATIONS'])\n",
        "if not os.path.isdir(paths['DATASET']):\n",
        "    os.mkdir(paths['DATASET'])\n",
        "if not os.path.isdir(paths['LOGODET_3K_FLAT_SUBSET']):\n",
        "    os.mkdir(paths['LOGODET_3K_FLAT_SUBSET'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odg2ZLlQEoGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a3d016-7edb-4143-ee72-3bba95099054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-08 12:06:06--  https://github.com/paul2048/annotated-images/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/paul2048/annotated-images/zip/refs/heads/master [following]\n",
            "--2022-05-08 12:06:06--  https://codeload.github.com/paul2048/annotated-images/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [ <=>                ]   5.89K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-05-08 12:06:06 (3.56 MB/s) - ‘master.zip’ saved [6027]\n",
            "\n",
            "Archive:  /content/master.zip\n",
            "aabb7fea70287f6c782d49374d898acc2ab1065a\n",
            "   creating: annotated-images-master/\n",
            "  inflating: annotated-images-master/.gitignore  \n",
            "  inflating: annotated-images-master/LICENSE  \n",
            "  inflating: annotated-images-master/README.md  \n",
            "   creating: annotated-images-master/annotated_images/\n",
            "  inflating: annotated-images-master/annotated_images/__init__.py  \n",
            "  inflating: annotated-images-master/annotated_images/parse_tags.py  \n",
            "  inflating: annotated-images-master/annotated_images/split.py  \n",
            " extracting: annotated-images-master/requirements.txt  \n",
            "  inflating: annotated-images-master/setup.py  \n",
            "Processing ./annotated-images-master\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: annotated-images\n",
            "  Building wheel for annotated-images (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annotated-images: filename=annotated_images-0.1.5-py3-none-any.whl size=5129 sha256=231d133dd2d13b0fe1cb1fe175c7e3763ff40eb5ade2d993698c7ee3016d48c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/25/ae/a3b8d89617974cdbc6edceea120440d05e187110ef53a40cba\n",
            "Successfully built annotated-images\n",
            "Installing collected packages: annotated-images\n",
            "Successfully installed annotated-images-0.1.5\n",
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 72697, done.\u001b[K\n",
            "remote: Counting objects: 100% (206/206), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 72697 (delta 82), reused 161 (delta 59), pack-reused 72491\u001b[K\n",
            "Receiving objects: 100% (72697/72697), 579.29 MiB | 29.25 MiB/s, done.\n",
            "Resolving deltas: 100% (51373/51373), done.\n"
          ]
        }
      ],
      "source": [
        "# Install a package that splits the dataset in train and test sets\n",
        "!wget https://github.com/paul2048/annotated-images/archive/refs/heads/master.zip\n",
        "!unzip {ROOT_PATH}/master.zip\n",
        "!pip install {ROOT_PATH}/annotated-images-master\n",
        "!rm -rf annotated-images-master master.zip\n",
        "!rm -rf sample_data\n",
        "# Clone the TensorFlow Model Garden repository\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "!mv models {ROOT_PATH}/{TFOD_DIR_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VLybQoVqkY1"
      },
      "source": [
        "##Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8amyRveUQeL9",
        "outputId": "21b15cfb-3096-4a0f-c8f7-621c51e3e5e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "drive.mount(paths['DRIVE'])\n",
        "shutil.unpack_archive(files['LOGODET_3K'], paths['DATASET'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Qg25XvNCXW"
      },
      "outputs": [],
      "source": [
        "# Logo names used as classes for the model that we will build\n",
        "selected_logos = ['Apple', 'ASUS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLrIvwERaRva",
        "outputId": "e84e1854-14cd-42aa-ce8a-7d3c98951d22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': 50, 'total': 165, 'train': 115}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import annotated_images\n",
        "\n",
        "# Delete the test and train sets of the previous split\n",
        "trainset_path = os.path.join(paths['DATASET'], 'train')\n",
        "testset_path = os.path.join(paths['DATASET'], 'test')\n",
        "if os.path.isdir(trainset_path):\n",
        "    shutil.rmtree(trainset_path)\n",
        "if os.path.isdir(testset_path):\n",
        "    shutil.rmtree(testset_path)\n",
        "\n",
        "for category in os.listdir(paths['LOGODET_3K']):\n",
        "    brands_path = os.path.join(paths['LOGODET_3K'], category)\n",
        "    # Loop over the brands (7-Up, Apple, etc.)\n",
        "    for brand in os.listdir(brands_path):\n",
        "        if brand in selected_logos:\n",
        "            brand_path = os.path.join(brands_path, brand)\n",
        "            # Loop over each file (1.jpg, 1.xml, etc.)\n",
        "            for file_ in os.listdir(brand_path):\n",
        "                source_path = os.path.join(brand_path, file_)\n",
        "                # Append the name of the brand at the beginning of the file name\n",
        "                # because \"1.jpg\" (for example) repeats at each brand\n",
        "                if file_.endswith('.xml'):\n",
        "                    with open(source_path) as f:\n",
        "                        tree = ET.parse(f)\n",
        "                        root = tree.getroot()\n",
        "                        xml_filename = root.find('filename')\n",
        "                        num, extenssion = xml_filename.text.split('.')\n",
        "                        num = num.split('_')[-1]\n",
        "                        xml_filename.text = f'{brand}_{num}.{extenssion}'\n",
        "                    tree.write(source_path)\n",
        "                destination_path = os.path.join(paths['LOGODET_3K_FLAT_SUBSET'], f'{brand}_{file_}')\n",
        "                shutil.copy(source_path, destination_path)\n",
        "\n",
        "annotated_images.split(paths['LOGODET_3K_FLAT_SUBSET'], output_dir=paths['DATASET'], ratio=(0.7, 0.3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH2vI9ZeIz43"
      },
      "source": [
        "## Download pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjlyUWobI4g6",
        "outputId": "98ce5983-24bc-4f0d-c188-fd24e0a10edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-08 12:59:50--  http://download.tensorflow.org/models/object_detection/tf2/20200713/centernet_hg104_512x512_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.152.128, 2607:f8b0:4001:c56::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.152.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1426099846 (1.3G) [application/x-tar]\n",
            "Saving to: ‘centernet_hg104_512x512_coco17_tpu-8.tar.gz’\n",
            "\n",
            "centernet_hg104_512 100%[===================>]   1.33G   135MB/s    in 9.1s    \n",
            "\n",
            "2022-05-08 12:59:59 (149 MB/s) - ‘centernet_hg104_512x512_coco17_tpu-8.tar.gz’ saved [1426099846/1426099846]\n",
            "\n",
            "centernet_hg104_512x512_coco17_tpu-8/\n",
            "centernet_hg104_512x512_coco17_tpu-8/checkpoint/\n",
            "centernet_hg104_512x512_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
            "centernet_hg104_512x512_coco17_tpu-8/checkpoint/checkpoint\n",
            "centernet_hg104_512x512_coco17_tpu-8/checkpoint/ckpt-0.index\n",
            "centernet_hg104_512x512_coco17_tpu-8/pipeline.config\n",
            "centernet_hg104_512x512_coco17_tpu-8/saved_model/\n",
            "centernet_hg104_512x512_coco17_tpu-8/saved_model/saved_model.pb\n",
            "centernet_hg104_512x512_coco17_tpu-8/saved_model/assets/\n",
            "centernet_hg104_512x512_coco17_tpu-8/saved_model/variables/\n",
            "centernet_hg104_512x512_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
            "centernet_hg104_512x512_coco17_tpu-8/saved_model/variables/variables.index\n"
          ]
        }
      ],
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200713/{PRETRAINED_MODEL_NAME}.tar.gz\n",
        "!tar -xvzf {PRETRAINED_MODEL_NAME}.tar.gz -C {ROOT_PATH}\n",
        "!rm {PRETRAINED_MODEL_NAME}.tar.gz\n",
        "# Rename the \"checkpoint\" directory (https://stackoverflow.com/a/64159833/7367049)\n",
        "os.rename(\n",
        "    os.path.join(paths['PRETRAINED_MODEL'], 'checkpoint'),\n",
        "    os.path.join(paths['PRETRAINED_MODEL'], 'checkpoint0'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzg67SZnIgNm"
      },
      "source": [
        "## Create label map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pXqKHgGpePSi",
        "outputId": "a1f69d8d-80b8-4a3b-dbc9-f5e08d2675c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'item {\\n    id: 1\\n    name: \"Apple\"\\n}\\nitem {\\n    id: 2\\n    name: \"ASUS\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "label_map = '\\n'.join([\n",
        "    f'item {{\\n    id: {i+1}\\n    name: \"{brand_name}\"\\n}}'\n",
        "    for i, brand_name in enumerate(selected_logos)\n",
        "])\n",
        "label_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33O9qBFRxYIW"
      },
      "outputs": [],
      "source": [
        "with open(files['LABEL_MAP'], 'w+') as f:\n",
        "    f.write(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T98c_aStIlOb"
      },
      "source": [
        "## Proto files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IDTwuJzV_8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3483d6fd-388d-44bb-db42-fcd8d4f13c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/tf_object_detection/research\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.38.0-cp37-cp37m-manylinux2010_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.28)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 31.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.4 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.8.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 50 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 47.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow-text~=2.8.0\n",
            "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 34.5 MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.6)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.31.5)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.56.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2022.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.10.8)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (4.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.25.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.44.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.8.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 42.4 MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 38.9 MB/s \n",
            "\u001b[?25hCollecting cloudpickle<3,>=2.0.0\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: pyarrow<7.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.8-cp37-cp37m-manylinux_2_24_x86_64.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 49.7 MB/s \n",
            "\u001b[?25hCollecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.4.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Collecting protobuf>=3.12.0\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.12)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.7.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.7.0)\n",
            "Building wheels for collected packages: object-detection, py-cpuinfo, dill, avro-python3, seqeval\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1692480 sha256=3b1d36468c54e49ca9ab9c57bcf321419778671b2853b6447d17b280d21cf7bb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-77h5bzz1/wheels/3c/4b/e1/00786286486d4034b80cd6ccc42763472c83f47e317abed512\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=1714684582c653ab5e3416318fcab591d306b7dc15070a3ed573217e418c5696\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=84fe35969b7a4f612fb55942fc0336fb324d6764d82791da2a15c2b1fe0fddfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=da52025548ccb9e095801cd404ae03f86dba68e25b76393aa1221dbc57c00d0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=a1c50ab3a03375798edff4342cb0b9b80afc185378b3c56cace5230916336e00\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built object-detection py-cpuinfo dill avro-python3 seqeval\n",
            "Installing collected packages: requests, protobuf, tf-estimator-nightly, portalocker, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, proto-plus, orjson, opencv-python-headless, hdfs, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.1.1\n",
            "    Uninstalling pymongo-4.1.1:\n",
            "      Successfully uninstalled pymongo-4.1.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.38.0 avro-python3-1.10.2 cloudpickle-2.0.0 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.11 hdfs-2.7.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.5.64 orjson-3.6.8 portalocker-2.4.0 proto-plus-1.20.3 protobuf-3.20.1 py-cpuinfo-8.0.0 pymongo-3.12.3 pyyaml-5.4.1 requests-2.27.1 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.16.1 tensorflow-io-0.25.0 tensorflow-model-optimization-0.7.2 tensorflow-text-2.8.2 tf-estimator-nightly-2.8.0.dev2021122109 tf-models-official-2.8.0 tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "# Protos list: https://github.com/tensorflow/models/tree/master/research/object_detection/protos\n",
        "!cd {TFOD_DIR_NAME}/research && \\\n",
        "    protoc object_detection/protos/*.proto --python_out=. && \\\n",
        "    cp object_detection/packages/tf2/setup.py . && \\\n",
        "    python -m pip install .\n",
        "import object_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzf2_1AEJfCu",
        "outputId": "c08ce0ee-4b7f-4385-9536-561e0f42bb4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-08 13:01:10--  https://github.com/sglvladi/TensorFlowObjectDetectionTutorial/raw/master/docs/source/scripts/generate_tfrecord.py\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sglvladi/TensorFlowObjectDetectionTutorial/master/docs/source/scripts/generate_tfrecord.py [following]\n",
            "--2022-05-08 13:01:10--  https://raw.githubusercontent.com/sglvladi/TensorFlowObjectDetectionTutorial/master/docs/source/scripts/generate_tfrecord.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6410 (6.3K) [text/plain]\n",
            "Saving to: ‘generate_tfrecord.py’\n",
            "\n",
            "generate_tfrecord.p 100%[===================>]   6.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-08 13:01:10 (57.0 MB/s) - ‘generate_tfrecord.py’ saved [6410/6410]\n",
            "\n",
            "/content\n",
            "Successfully created the TFRecord file: /content/annotations/train.record\n",
            "Successfully created the TFRecord file: /content/annotations/test.record\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/sglvladi/TensorFlowObjectDetectionTutorial/raw/master/docs/source/scripts/generate_tfrecord.py\n",
        "%cd {ROOT_PATH}\n",
        "!python generate_tfrecord.py -x {paths['TRAIN_SET']} -l {files['LABEL_MAP']} -o {os.path.join(paths['ANNOTATIONS'], 'train.record')}\n",
        "!python generate_tfrecord.py -x {paths['TEST_SET']} -l {files['LABEL_MAP']} -o {os.path.join(paths['ANNOTATIONS'], 'test.record')}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kqsAu3AYA-Q"
      },
      "source": [
        "## Config file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok2NqEVpmfYS"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import config_util\n",
        "from object_detection.protos import pipeline_pb2\n",
        "from google.protobuf import text_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNXQjBWimnHh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
        "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], 'r') as f:\n",
        "    proto_str = f.read()\n",
        "    text_format.Merge(proto_str, pipeline_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rlaNYkMmq2s"
      },
      "outputs": [],
      "source": [
        "pipeline_config.model.center_net.num_classes = len(selected_logos)\n",
        "pipeline_config.model.center_net.image_resizer.keep_aspect_ratio_resizer.min_dimension = 256\n",
        "pipeline_config.model.center_net.image_resizer.keep_aspect_ratio_resizer.max_dimension = 256\n",
        "pipeline_config.train_config.batch_size = 16\n",
        "pipeline_config.train_config.max_number_of_boxes = 25\n",
        "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL'], 'checkpoint0', 'ckpt-0')\n",
        "pipeline_config.train_input_reader.label_map_path = files['LABEL_MAP']\n",
        "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATIONS'], 'train.record')]\n",
        "pipeline_config.eval_input_reader[0].label_map_path = files['LABEL_MAP']\n",
        "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATIONS'], 'test.record')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND71eGrEmr-S"
      },
      "outputs": [],
      "source": [
        "config_text = text_format.MessageToString(pipeline_config)\n",
        "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], 'wb') as f:\n",
        "    f.write(config_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGVI59W3nQoy"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDKsvNjRBDMx",
        "outputId": "43aace1c-362c-41c0-a5e7-208825a44a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 19.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.6)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.5.5.64\n",
            "    Uninstalling opencv-python-headless-4.5.5.64:\n",
            "      Successfully uninstalled opencv-python-headless-4.5.5.64\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n"
          ]
        }
      ],
      "source": [
        "# Fixes \"ImportError: cannot import name '_registerMatType' from 'cv2.cv2'\" when training\n",
        "!pip install 'opencv-python-headless<4.3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ketm0c7sUy7V"
      },
      "outputs": [],
      "source": [
        "train_steps = 2000\n",
        "command = f'python {files[\"TRAINING_SCRIPT\"]} \\\n",
        "            --model_dir={paths[\"PRETRAINED_MODEL\"]} \\\n",
        "            --pipeline_config_path=\"{files[\"PIPELINE_CONFIG\"]}\" \\\n",
        "            --checkpoint_every_n={train_steps} \\\n",
        "            --num_train_steps={train_steps}'\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc9ACqmbzXwT"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqlpNBvuw2Vx"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {paths['PRETRAINED_MODEL']}/train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkEtqg5LxNQx"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tHZtpQJ6Br-"
      },
      "outputs": [],
      "source": [
        "command = f'python {files[\"TRAINING_SCRIPT\"]} \\\n",
        "            --model_dir={paths[\"PRETRAINED_MODEL\"]} \\\n",
        "            --pipeline_config_path={files[\"PIPELINE_CONFIG\"]} \\\n",
        "            --checkpoint_dir={paths[\"PRETRAINED_MODEL\"]}'\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WonP_JU75aR"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {paths['PRETRAINED_MODEL']}/eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QRXh9I_rOff"
      },
      "source": [
        "## Load the trained model from checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aSrnGhDZrR-X"
      },
      "outputs": [],
      "source": [
        "from object_detection.builders import model_builder\n",
        "from object_detection.utils import config_util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzWYeIFQrvK3"
      },
      "outputs": [],
      "source": [
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
        "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(os.path.join(paths['PRETRAINED_MODEL'], 'ckpt-22')).expect_partial()\n",
        "\n",
        "@tf.function\n",
        "def detect_fn(image):\n",
        "    image, shapes = detection_model.preprocess(image)\n",
        "    prediction_dict = detection_model.predict(image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iTieX1Qrwrc"
      },
      "source": [
        "## Test image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0OlK_r2r563"
      },
      "outputs": [],
      "source": [
        "import cv2 \n",
        "import numpy as np\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpBEd8Tkr-wM"
      },
      "outputs": [],
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(files['LABEL_MAP'])\n",
        "IMAGE_PATH = '/content/30.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LacnhyesJjw"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(IMAGE_PATH)\n",
        "image_np = np.array(img)\n",
        "\n",
        "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "num_detections = int(detections.pop('num_detections'))\n",
        "detections = {key: value[0, :num_detections].numpy()\n",
        "              for key, value in detections.items()}\n",
        "detections['num_detections'] = num_detections\n",
        "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "label_id_offset = 1\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "            image_np_with_detections,\n",
        "            detections['detection_boxes'],\n",
        "            detections['detection_classes'] + label_id_offset,\n",
        "            detections['detection_scores'],\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            max_boxes_to_draw=5,\n",
        "            min_score_thresh=.8,\n",
        "            agnostic_mode=False)\n",
        "\n",
        "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lkEtqg5LxNQx",
        "9QRXh9I_rOff"
      ],
      "machine_shape": "hm",
      "name": "Logo_detection_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}